{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4\n",
    "\n",
    "We will use pre-populated accelerometer data to learn how feature engineering boosts model performance.\n",
    "We will use Apache SystemML to implement Discrete Fourier Transformation on the accelerometer sensor time series, therefore transforming the dataset from the time to the frequency domain.\n",
    "Then, we will train an Apache SparkML classification model to obtain model predictions on the transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache SystemML (now renamed as SystemDS) is a system that accelerates exploratory algorithm development for large-scale machine learning problems. SystemML provides a high-level language to quickly implement and run machine learning algorithms on Spark. SystemML's cost-based optimizer takes care of low-level decisions about how to use Spark’s parallelism, allowing users to focus on the algorithm and the real-world problem that the algorithm is trying to solve. \n",
    "\n",
    "See more here:\n",
    "https://databricks.com/session/inside-apache-systemml\n",
    "https://researcher.watson.ibm.com/researcher/view_group.php?id=3174"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting pyspark==2.4.5\n",
      "  Downloading pyspark-2.4.5.tar.gz (217.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 217.8 MB 11 kB/s s eta 0:00:01��██▍     | 179.6 MB 33.0 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting py4j==0.10.7\n",
      "  Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 89.0 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257928 sha256=a375236b7a0a6aa92c9852e39d3be4939830f768f9470e8d1b0a2ba40215cb7b\n",
      "  Stored in directory: /tmp/wsuser/.cache/pip/wheels/01/c0/03/1c241c9c482b647d4d99412a98a5c7f87472728ad41ae55e1e\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark==2.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting https://github.com/IBM/coursera/blob/master/systemml-1.3.0-SNAPSHOT-python.tar.gz?raw=true\n",
      "  Downloading https://github.com/IBM/coursera/blob/master/systemml-1.3.0-SNAPSHOT-python.tar.gz?raw=true (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 20.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.8.2 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from systemml==1.3.0) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.15.1 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from systemml==1.3.0) (1.4.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from systemml==1.3.0) (1.0.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from systemml==1.3.0) (0.23.2)\n",
      "Requirement already satisfied: Pillow>=2.0.0 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from systemml==1.3.0) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from pandas->systemml==1.3.0) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from pandas->systemml==1.3.0) (2020.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from scikit-learn->systemml==1.3.0) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from scikit-learn->systemml==1.3.0) (0.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas->systemml==1.3.0) (1.15.0)\n",
      "Building wheels for collected packages: systemml\n",
      "  Building wheel for systemml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for systemml: filename=systemml-1.3.0-py3-none-any.whl size=9882972 sha256=edf4594520c9c890eb66674aa9048d10be9749162195ccf4f7d50d4bbeaa73dc\n",
      "  Stored in directory: /tmp/wsuser/.cache/pip/wheels/ed/96/15/1042ed0087d53c21a17788d99d5581169482cfe683f1f6e60a\n",
      "Successfully built systemml\n",
      "Installing collected packages: systemml\n",
      "Successfully installed systemml-1.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/IBM/coursera/blob/master/systemml-1.3.0-SNAPSHOT-python.tar.gz?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So the first thing we need to ensure is that we are on the latest version of SystemML, which is 1.3.0 (as of 20th March'19) Please use the code block below to check if you are already on 1.3.0 or higher. 1.3 contains a necessary fix, that's we are running against the SNAPSHOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/dsxuser’: Permission denied\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /home/dsxuser/work/systemml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0-SNAPSHOT\n"
     ]
    }
   ],
   "source": [
    "from systemml import MLContext, dml\n",
    "ml = MLContext(spark)\n",
    "ml.setConfigProperty(\"sysml.localtmpdir\", \"mkdir /home/dsxuser/work/systemml\")\n",
    "print(ml.version())\n",
    "    \n",
    "if not ml.version() == '1.3.0-SNAPSHOT':\n",
    "    raise ValueError('please upgrade to SystemML 1.3.0, or restart your Kernel (Kernel->Restart & Clear Output)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-05 21:08:12--  https://github.com/IBM/coursera/blob/master/coursera_ml/shake.parquet?raw=true\n",
      "Resolving github.com (github.com)... 140.82.112.4\n",
      "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://github.com/IBM/claimed/blob/master/coursera_ml/shake.parquet?raw=true [following]\n",
      "--2021-11-05 21:08:12--  https://github.com/IBM/claimed/blob/master/coursera_ml/shake.parquet?raw=true\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/IBM/claimed/raw/master/coursera_ml/shake.parquet [following]\n",
      "--2021-11-05 21:08:12--  https://github.com/IBM/claimed/raw/master/coursera_ml/shake.parquet\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/IBM/claimed/master/coursera_ml/shake.parquet [following]\n",
      "--2021-11-05 21:08:13--  https://raw.githubusercontent.com/IBM/claimed/master/coursera_ml/shake.parquet\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 74727 (73K) [application/octet-stream]\n",
      "Saving to: ‘shake.parquet?raw=true’\n",
      "\n",
      "shake.parquet?raw=t 100%[===================>]  72.98K  --.-KB/s    in 0.001s  \n",
      "\n",
      "2021-11-05 21:08:13 (88.9 MB/s) - ‘shake.parquet?raw=true’ saved [74727/74727]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/IBM/coursera/blob/master/coursera_ml/shake.parquet?raw=true\n",
    "!mv shake.parquet?raw=true shake.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to read the sensor data and create a temporary query table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('shake.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+-----+-----+\n",
      "|CLASS| SENSORID|    X|    Y|    Z|\n",
      "+-----+---------+-----+-----+-----+\n",
      "|    2| qqqqqqqq| 0.12| 0.12| 0.12|\n",
      "|    2|aUniqueID| 0.03| 0.03| 0.03|\n",
      "|    2| qqqqqqqq|-3.84|-3.84|-3.84|\n",
      "|    2| 12345678| -0.1| -0.1| -0.1|\n",
      "|    2| 12345678|-0.15|-0.15|-0.15|\n",
      "|    2| 12345678| 0.47| 0.47| 0.47|\n",
      "|    2| 12345678|-0.06|-0.06|-0.06|\n",
      "|    2| 12345678|-0.09|-0.09|-0.09|\n",
      "|    2| 12345678| 0.21| 0.21| 0.21|\n",
      "|    2| 12345678|-0.08|-0.08|-0.08|\n",
      "|    2| 12345678| 0.44| 0.44| 0.44|\n",
      "|    2|    gholi| 0.76| 0.76| 0.76|\n",
      "|    2|    gholi| 1.62| 1.62| 1.62|\n",
      "|    2|    gholi| 5.81| 5.81| 5.81|\n",
      "|    2| bcbcbcbc| 0.58| 0.58| 0.58|\n",
      "|    2| bcbcbcbc|-8.24|-8.24|-8.24|\n",
      "|    2| bcbcbcbc|-0.45|-0.45|-0.45|\n",
      "|    2| bcbcbcbc| 1.03| 1.03| 1.03|\n",
      "|    2|aUniqueID|-0.05|-0.05|-0.05|\n",
      "|    2| qqqqqqqq|-0.44|-0.44|-0.44|\n",
      "+-----+---------+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting pixiedust\n",
      "  Downloading pixiedust-1.1.19.tar.gz (197 kB)\n",
      "\u001b[K     |████████████████████████████████| 197 kB 28.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting geojson\n",
      "  Downloading geojson-2.5.0-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: astunparse in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from pixiedust) (1.6.3)\n",
      "Requirement already satisfied: markdown in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from pixiedust) (3.1.1)\n",
      "Collecting colour\n",
      "  Downloading colour-0.1.5-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from pixiedust) (2.25.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from pixiedust) (3.2.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from pixiedust) (1.0.5)\n",
      "Requirement already satisfied: six<2.0,>=1.6.1 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from astunparse->pixiedust) (1.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from astunparse->pixiedust) (0.35.1)\n",
      "Requirement already satisfied: setuptools>=36 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from markdown->pixiedust) (47.3.1.post20200622)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from requests->pixiedust) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from requests->pixiedust) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from requests->pixiedust) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from requests->pixiedust) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from matplotlib->pixiedust) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from matplotlib->pixiedust) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from matplotlib->pixiedust) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from matplotlib->pixiedust) (1.19.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from matplotlib->pixiedust) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages (from pandas->pixiedust) (2020.1)\n",
      "Building wheels for collected packages: pixiedust\n",
      "  Building wheel for pixiedust (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pixiedust: filename=pixiedust-1.1.19-py3-none-any.whl size=321803 sha256=9a8f2ce0bcbb0a773167d5955168bd84007e324f53566f434cfa6755fe916c6f\n",
      "  Stored in directory: /tmp/wsuser/.cache/pip/wheels/05/07/e7/8aca0e820027a63157a916424fd748fb2a2a3e71de5e08eeb8\n",
      "Successfully built pixiedust\n",
      "Installing collected packages: geojson, colour, pixiedust\n",
      "Successfully installed colour-0.1.5 geojson-2.5.0 pixiedust-1.1.19\n"
     ]
    }
   ],
   "source": [
    "!pip install pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n",
      "Table VERSION_TRACKER created successfully\n",
      "Table METRICS_TRACKER created successfully\n",
      "\n",
      "Share anonymous install statistics? (opt-out instructions)\n",
      "\n",
      "PixieDust will record metadata on its environment the next time the package is installed or updated. The data is anonymized and aggregated to help plan for future releases, and records only the following values:\n",
      "\n",
      "{\n",
      "   \"data_sent\": currentDate,\n",
      "   \"runtime\": \"python\",\n",
      "   \"application_version\": currentPixiedustVersion,\n",
      "   \"space_id\": nonIdentifyingUniqueId,\n",
      "   \"config\": {\n",
      "       \"repository_id\": \"https://github.com/ibm-watson-data-lab/pixiedust\",\n",
      "       \"target_runtimes\": [\"Data Science Experience\"],\n",
      "       \"event_id\": \"web\",\n",
      "       \"event_organizer\": \"dev-journeys\"\n",
      "   }\n",
      "}\n",
      "You can opt out by calling pixiedust.optOut() in a new cell.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.19</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPixiedust runtime updated. Please restart kernel\u001b[0m\n",
      "Table SPARK_PACKAGES created successfully\n",
      "Table USER_PREFERENCES created successfully\n",
      "Table service_connections created successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[CLASS: bigint, SENSORID: string, X: double, Y: double, Z: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pixiedust\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use Apache SystemML to implement Discrete Fourier Transformation. This way all computation continues to happen on the Apache Spark cluster for advanced scalability and performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you’ve learned from the lecture, implementing Discrete Fourier Transformation in a linear algebra programming language is simple. Apache SystemML DML is such a language and as you can see the implementation is straightforward and doesn’t differ too much from the mathematical definition (Just note that the sum operator has been swapped with a vector dot product using the %*% syntax borrowed from R\n",
    "):\n",
    "\n",
    "<img style=\"float: left;\" src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1af0a78dc50bbf118ab6bd4c4dcc3c4ff8502223\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_script = '''\n",
    "PI = 3.141592654\n",
    "N = nrow(signal)\n",
    "\n",
    "n = seq(0, N-1, 1)\n",
    "k = seq(0, N-1, 1)\n",
    "\n",
    "M = (n %*% t(k))*(2*PI/N)\n",
    "\n",
    "Xa = cos(M) %*% signal\n",
    "Xb = sin(M) %*% signal\n",
    "\n",
    "DFT = cbind(Xa, Xb)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to create a function which takes a single row Apache Spark data frame as argument (the one containing the accelerometer measurement time series for one axis) and returns the Fourier transformation of it. In addition, we are adding an index column for later joining all axis together and renaming the columns to appropriate names. The result of this function is an Apache Spark DataFrame containing the Fourier Transformation of its input in two columns. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "def dft_systemml(signal, name):\n",
    "    prog = dml(dml_script).input('signal', signal).output('DFT')\n",
    "    \n",
    "    return (\n",
    "\n",
    "    #execute the script inside the SystemML engine running on top of Apache Spark\n",
    "    ml.execute(prog) \n",
    "     \n",
    "         #read result from SystemML execution back as SystemML Matrix\n",
    "        .get('DFT') \n",
    "     \n",
    "         #convert SystemML Matrix to ApacheSpark DataFrame \n",
    "        .toDF() \n",
    "     \n",
    "         #rename default column names\n",
    "        .selectExpr('C1 as %sa' % (name), 'C2 as %sb' % (name)) \n",
    "     \n",
    "         #add unique ID per row for later joining\n",
    "        .withColumn(\"id\", monotonically_increasing_id())\n",
    "    )\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to create individual DataFrames containing only a subset of the data. We filter simultaneously for accelerometer each sensor axis and one for each class. This means we’ll get 6 DataFrames. We can implement this using the relational API of DataFrames or SparkSQL. \n",
    "Here we should use class 1 and 2 and not 0 and 1. Let's make sure that each DataFrame has only ONE colum (only the measurement, eg. not CLASS column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(\"select x from df where class = 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = spark.sql(\"select x from df where class = 1\") # create a DataFrame containing only measurements of class 0 from the x axis\n",
    "y0 = spark.sql(\"select y from df where class = 1\") # create a DataFrame containing only measurements of class 0 from the y axis\n",
    "z0 = spark.sql(\"select z from df where class = 1\") # create a DataFrame containing only measurements of class 0 from the z axis\n",
    "x1 = spark.sql(\"select x from df where class = 2\") # create a DataFrame containing only measurements of class 1 from the x axis\n",
    "y1 = spark.sql(\"select y from df where class = 2\") # create a DataFrame containing only measurements of class 1 from the y axis\n",
    "z1 = spark.sql(\"select z from df where class = 2\") # create a DataFrame containing only measurements of class 1 from the z axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we’ve created this cool DFT function before, we can just call it for each of the 6 DataFrames now. And since the result of this function call is a DataFrame again we can use the pyspark best practice in simply calling methods on it sequentially. So what we are doing is the following:\n",
    "\n",
    "- Calling DFT for each class and accelerometer sensor axis.\n",
    "- Joining them together on the ID column. \n",
    "- Re-adding a column containing the class index.\n",
    "- Stacking both Dataframes for each classes together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemML Statistics:\n",
      "Total execution time:\t\t0.470 sec.\n",
      "Number of executed Spark inst:\t0.\n",
      "\n",
      "\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t0.221 sec.\n",
      "Number of executed Spark inst:\t0.\n",
      "\n",
      "\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t0.192 sec.\n",
      "Number of executed Spark inst:\t0.\n",
      "\n",
      "\n",
      "[Stage 21:>                                                         (0 + 1) / 1]\n",
      "[Stage 23:>                                                         (0 + 1) / 1]\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t14.966 sec.\n",
      "Number of executed Spark inst:\t6.\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 29:>                                                         (0 + 1) / 1]\n",
      "                                                                                \n",
      "[Stage 31:>                                                         (0 + 1) / 1]\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t11.022 sec.\n",
      "Number of executed Spark inst:\t6.\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 37:>                                                         (0 + 1) / 1]\n",
      "                                                                                \n",
      "[Stage 39:>                                                         (0 + 1) / 1]\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t10.910 sec.\n",
      "Number of executed Spark inst:\t6.\n",
      "\n",
      "                                                                                \n",
      "\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| id|                  xa|                  xb|                  ya|                  yb|                  za|                  zb|class|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "| 26| 0.03912775214058598|-0.09049016952668232| 0.03912775214058598|-0.09049016952668232| 0.03912775214058598|-0.09049016952668232|    0|\n",
      "| 29|0.006260524476137005|-0.05765058448048809|0.006260524476137005|-0.05765058448048809|0.006260524476137005|-0.05765058448048809|    0|\n",
      "| 65| -0.0686442877106673| 0.06538926077517343| -0.0686442877106673| 0.06538926077517343| -0.0686442877106673| 0.06538926077517343|    0|\n",
      "| 19| 0.07207851982267682|-0.00555713532424...| 0.07207851982267682|-0.00555713532424...| 0.07207851982267682|-0.00555713532424...|    0|\n",
      "| 54| 0.03806782602909542|-0.15673003795074777| 0.03806782602909542|-0.15673003795074777| 0.03806782602909542|-0.15673003795074777|    0|\n",
      "|  0| 0.06178937294558884|-0.03961749197816182| 0.06178937294558884|-0.03961749197816182| 0.06178937294558884|-0.03961749197816182|    0|\n",
      "|112|0.050945537592681445|-0.00299428645504...|0.050945537592681445|-0.00299428645504...|0.050945537592681445|-0.00299428645504...|    0|\n",
      "|113| 0.06178937118785787| 0.03961749600785146| 0.06178937118785787| 0.03961749600785146| 0.06178937118785787| 0.03961749600785146|    0|\n",
      "| 22|-0.00942353499552...|0.014489276203582657|-0.00942353499552...|0.014489276203582657|-0.00942353499552...|0.014489276203582657|    0|\n",
      "|  7|-0.02139526027010...| 0.11560580654216471|-0.02139526027010...| 0.11560580654216471|-0.02139526027010...| 0.11560580654216471|    0|\n",
      "| 77|-0.07768949082949561|0.039133460895471164|-0.07768949082949561|0.039133460895471164|-0.07768949082949561|0.039133460895471164|    0|\n",
      "| 34|-0.08799817666162946|0.017746602649508163|-0.08799817666162946|0.017746602649508163|-0.08799817666162946|0.017746602649508163|    0|\n",
      "| 50| -0.0189063133643019|-0.08871621465767468| -0.0189063133643019|-0.08871621465767468| -0.0189063133643019|-0.08871621465767468|    0|\n",
      "| 94|0.007432297488510396|0.003946633565674247|0.007432297488510396|0.003946633565674247|0.007432297488510396|0.003946633565674247|    0|\n",
      "|110|  0.0334907666040125|  0.0703175032167542|  0.0334907666040125|  0.0703175032167542|  0.0334907666040125|  0.0703175032167542|    0|\n",
      "| 57|0.039127744325949065| 0.09049016873590601|0.039127744325949065| 0.09049016873590601|0.039127744325949065| 0.09049016873590601|    0|\n",
      "| 32|-0.13783372672389743|  0.0630534416705046|-0.13783372672389743|  0.0630534416705046|-0.13783372672389743|  0.0630534416705046|    0|\n",
      "| 43| 0.04362147516153375|-0.04661273772305589| 0.04362147516153375|-0.04661273772305589| 0.04362147516153375|-0.04661273772305589|    0|\n",
      "| 84|0.041318959515210835|-0.02405618890046...|0.041318959515210835|-0.02405618890046...|0.041318959515210835|-0.02405618890046...|    0|\n",
      "| 31| -0.0213952544564988| -0.1156058071676539| -0.0213952544564988| -0.1156058071676539| -0.0213952544564988| -0.1156058071676539|    0|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "df_class_0 = dft_systemml(x0,'x') \\\n",
    "    .join(dft_systemml(y0,'y'), on=['id'], how='inner') \\\n",
    "    .join(dft_systemml(z0,'z'), on=['id'], how='inner') \\\n",
    "    .withColumn('class', lit(0))\n",
    "    \n",
    "df_class_1 = dft_systemml(x1,'x') \\\n",
    "    .join(dft_systemml(y1,'y'), on=['id'], how='inner') \\\n",
    "    .join(dft_systemml(z1,'z'), on=['id'], how='inner') \\\n",
    "    .withColumn('class', lit(1))\n",
    "\n",
    "df_dft = df_class_0.union(df_class_1)\n",
    "\n",
    "df_dft.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a VectorAssembler which consumes the newly created DFT columns and produces a column “features”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=[\"xa\",\"xb\",\"ya\",\"yb\",\"za\",\"zb\"],\n",
    "                                  outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a classifier from the SparkML package and assign it to the classifier variable. Let's make sure to set the “class” column as target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GBTClassifier(labelCol=\"class\", featuresCol=\"features\", maxIter=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s train and evaluate…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[vectorAssembler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df_dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(df_dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "| id|                  xa|                  xb|                  ya|                  yb|                  za|                  zb|class|            features|       rawPrediction|         probability|prediction|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "| 26| 0.03912775214058598|-0.09049016952668232| 0.03912775214058598|-0.09049016952668232| 0.03912775214058598|-0.09049016952668232|    0|[0.03912775214058...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "| 29|0.006260524476137005|-0.05765058448048809|0.006260524476137005|-0.05765058448048809|0.006260524476137005|-0.05765058448048809|    0|[0.00626052447613...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "| 65| -0.0686442877106673| 0.06538926077517343| -0.0686442877106673| 0.06538926077517343| -0.0686442877106673| 0.06538926077517343|    0|[-0.0686442877106...|[0.83002855223973...|[0.84024566850429...|       0.0|\n",
      "| 19| 0.07207851982267682|-0.00555713532424...| 0.07207851982267682|-0.00555713532424...| 0.07207851982267682|-0.00555713532424...|    0|[0.07207851982267...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "| 54| 0.03806782602909542|-0.15673003795074777| 0.03806782602909542|-0.15673003795074777| 0.03806782602909542|-0.15673003795074777|    0|[0.03806782602909...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|  0| 0.06178937294558884|-0.03961749197816182| 0.06178937294558884|-0.03961749197816182| 0.06178937294558884|-0.03961749197816182|    0|[0.06178937294558...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|112|0.050945537592681445|-0.00299428645504...|0.050945537592681445|-0.00299428645504...|0.050945537592681445|-0.00299428645504...|    0|[0.05094553759268...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|113| 0.06178937118785787| 0.03961749600785146| 0.06178937118785787| 0.03961749600785146| 0.06178937118785787| 0.03961749600785146|    0|[0.06178937118785...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "| 22|-0.00942353499552...|0.014489276203582657|-0.00942353499552...|0.014489276203582657|-0.00942353499552...|0.014489276203582657|    0|[-0.0094235349955...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|  7|-0.02139526027010...| 0.11560580654216471|-0.02139526027010...| 0.11560580654216471|-0.02139526027010...| 0.11560580654216471|    0|[-0.0213952602701...|[0.83002855223973...|[0.84024566850429...|       0.0|\n",
      "| 77|-0.07768949082949561|0.039133460895471164|-0.07768949082949561|0.039133460895471164|-0.07768949082949561|0.039133460895471164|    0|[-0.0776894908294...|[0.83002855223973...|[0.84024566850429...|       0.0|\n",
      "| 34|-0.08799817666162946|0.017746602649508163|-0.08799817666162946|0.017746602649508163|-0.08799817666162946|0.017746602649508163|    0|[-0.0879981766616...|[0.83002855223973...|[0.84024566850429...|       0.0|\n",
      "| 50| -0.0189063133643019|-0.08871621465767468| -0.0189063133643019|-0.08871621465767468| -0.0189063133643019|-0.08871621465767468|    0|[-0.0189063133643...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "| 94|0.007432297488510396|0.003946633565674247|0.007432297488510396|0.003946633565674247|0.007432297488510396|0.003946633565674247|    0|[0.00743229748851...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|110|  0.0334907666040125|  0.0703175032167542|  0.0334907666040125|  0.0703175032167542|  0.0334907666040125|  0.0703175032167542|    0|[0.03349076660401...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "| 57|0.039127744325949065| 0.09049016873590601|0.039127744325949065| 0.09049016873590601|0.039127744325949065| 0.09049016873590601|    0|[0.03912774432594...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "| 32|-0.13783372672389743|  0.0630534416705046|-0.13783372672389743|  0.0630534416705046|-0.13783372672389743|  0.0630534416705046|    0|[-0.1378337267238...|[0.83002855223973...|[0.84024566850429...|       0.0|\n",
      "| 43| 0.04362147516153375|-0.04661273772305589| 0.04362147516153375|-0.04661273772305589| 0.04362147516153375|-0.04661273772305589|    0|[0.04362147516153...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "| 84|0.041318959515210835|-0.02405618890046...|0.041318959515210835|-0.02405618890046...|0.041318959515210835|-0.02405618890046...|    0|[0.04131895951521...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "| 31| -0.0213952544564988| -0.1156058071676539| -0.0213952544564988| -0.1156058071676539| -0.0213952544564988| -0.1156058071676539|    0|[-0.0213952544564...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9981761070017225"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "binEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\").setPredictionCol(\"prediction\").setLabelCol(\"class\")\n",
    "binEval.evaluate(prediction) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -Rf a2_m4.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.repartition(1)\n",
    "prediction.write.json('a2_m4.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

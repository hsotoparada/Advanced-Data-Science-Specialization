{"cells": [{"metadata": {}, "cell_type": "code", "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n\n\nif ('sc' in locals() or 'sc' in globals()):\n    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')\n", "execution_count": 1, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install pyspark==2.4.5", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\n/opt/conda/envs/Python-3.7-OpenCE/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n  from cryptography.utils import int_from_bytes\nCollecting pyspark==2.4.5\n  Downloading pyspark-2.4.5.tar.gz (217.8 MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 217.8 MB 6.2 kB/s  eta 0:00:01    |\u2588\u2588\u2588\u2588\u2588\u258a                          | 39.0 MB 9.2 MB/s eta 0:00:20\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e           | 138.1 MB 26.4 MB/s eta 0:00:04     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d          | 145.7 MB 22.0 MB/s eta 0:00:04     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f  | 198.7 MB 21.1 MB/s eta 0:00:01\n\u001b[?25hCollecting py4j==0.10.7\n  Downloading py4j-0.10.7-py2.py3-none-any.whl (197 kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 197 kB 29.1 MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257928 sha256=3330016e086feba92030bccad84d1f7010389ec52e1e60923a6b54ad218c4654\n  Stored in directory: /tmp/wsuser/.cache/pip/wheels/01/c0/03/1c241c9c482b647d4d99412a98a5c7f87472728ad41ae55e1e\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.5\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "# Exercise 2\n## Part 1\nNow let's calculate covariance and correlation by ourselves using ApacheSpark\n\n1st we crate two random RDD\u2019s, which shouldn't correlate at all.\n"}, {"metadata": {}, "cell_type": "code", "source": "import random\nrddX = sc.parallelize(random.sample(list(range(100)),100))\nrddY = sc.parallelize(random.sample(list(range(100)),100))", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now we calculate the mean, note that we explicitly cast the denominator to float in order to obtain a float instead of int"}, {"metadata": {}, "cell_type": "code", "source": "meanX = rddX.sum()/float(rddX.count())\nmeanY = rddY.sum()/float(rddY.count())\nprint (meanX)\nprint (meanY)", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "49.5\n49.5\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Now we calculate the covariance"}, {"metadata": {}, "cell_type": "code", "source": "rddXY = rddX.zip(rddY)\ncovXY = rddXY.map(lambda x_y : (x_y[0]-meanX)*(x_y[1]-meanY)).sum()/rddXY.count()\ncovXY", "execution_count": 8, "outputs": [{"output_type": "execute_result", "execution_count": 8, "data": {"text/plain": "80.04"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "Covariance is not a normalized measure. Therefore we use it to calculate correlation. But before that we need to calculate the indivicual standard deviations first"}, {"metadata": {}, "cell_type": "code", "source": "from math import sqrt\n\nn = rddXY.count()\nsdX = sqrt(rddX.map(lambda x : pow(x-meanX,2)).sum()/n)\nsdY = sqrt(rddY.map(lambda x : pow(x-meanY,2)).sum()/n)\nprint (sdX)\nprint (sdY)", "execution_count": 9, "outputs": [{"output_type": "stream", "text": "28.86607004772212\n28.86607004772212\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Now we calculate the correlation"}, {"metadata": {}, "cell_type": "code", "source": "corrXY = covXY / (sdX * sdY)\ncorrXY", "execution_count": 10, "outputs": [{"output_type": "execute_result", "execution_count": 10, "data": {"text/plain": "0.09605760576057606"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "markdown", "source": "## Part 2\nNo we want to create a correlation matrix out of the four RDDs used in the lecture"}, {"metadata": {}, "cell_type": "code", "source": "from pyspark.mllib.stat import Statistics\nimport random\n\ncolumn1 = sc.parallelize(range(100))\ncolumn2 = sc.parallelize(range(100,200))\ncolumn3 = sc.parallelize(list(reversed(range(100))))\ncolumn4 = sc.parallelize(random.sample(range(100),100))\ndata = column1.zip(column2).zip(column3).zip(column4).map(lambda a_b_c_d : (a_b_c_d[0][0][0],a_b_c_d[0][0][1],a_b_c_d[0][1],a_b_c_d[1]) ).map(lambda a_b_c_d : [a_b_c_d[0],a_b_c_d[1],a_b_c_d[2],a_b_c_d[3]])\nprint(Statistics.corr(data))", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "[[ 1.          1.         -1.          0.13172517]\n [ 1.          1.         -1.          0.13172517]\n [-1.         -1.          1.         -0.13172517]\n [ 0.13172517  0.13172517 -0.13172517  1.        ]]\n", "name": "stdout"}]}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.11", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}